= InstructLab Playground
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2024-09
// Metadata
:description: This repository is my playground to learn what is `Instructlab` and how can I get the most out of it!
:keywords: openshift, red hat, rhoai, instructlab, ai, rhel
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]
// End: Enable admonition icons

This repository is my playground to learn what is `Instructlab` and how can I get the most out of it!


// Create the Table of contents here
toc::[]

== Introduction

*InstructLab* is an open source project that provides a platform for easy engagement with AI Large Language Models (LLM) by using the ilab command-line interface (CLI) tool.

Red Hat Enterprise Linux AI is distributed and installable as a bootable image. This bootable image includes a container that hold various software and tools for RHEL AI.

NOTE: This repository is based on the open-source Instructlab, but it will follow the downstream documentation from RHEL AI. If you are interested in support and guidance from Red Hat, I recommend you to check the https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.2[official documentation].



// A *large language model (LLM)* is a type of artificial intelligence (AI) model that uses deep learning techniques to understand and generate human-like text based on input data.




== Deploy RHEL AI on AWS

Even if you have an NVIDIA GPU on your laptop, you will be interested at some point to use other more powerful GPUs to fine-tune and deploy your LLMs. For that purpose, I have created a straight forward automation of the official documentation to upload the RHEL AI image as an AMI, in order to use it as a VM. The process consists of two simple steps:


=== Step 0: AWS credentials

First, you will need to create an `aws-env-vars` file and fill the following variables:

[source, bash]
----
# CHECK VARIABLE VALUES ARE BETWEEN DOUBLE QUOTES!
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export AWS_DEFAULT_REGION=""

export RHEL_AI_RAW_IMAGE_URL="" # The URL to download the RAW version of the RHEL AI Image
----

Then, you just need to source it to use the variables in the Ansible Playbook:

[source, bash]
----
source ./aws-env-vars
----

=== Step 1: Create an SSH Key pair


[source, bash]
----
ansible-playbook -vv playbooks/aws_keypair_create.yml
----


=== Step 2: Convert the RHEL AI Image to an AWS AMI

Before deploying the RHEL VM, we need to download the RHEL AI Raw image and push it to AWS to create an AMI:

[source, bash]
----
ansible-playbook -vv playbooks/aws_ec2_create_rhelai_ami.yml
----


=== Step 3: Deploy an instance on AWS

Finally, create an RHEL AI instance on AWS using the following Ansible Playbook:

[source, bash]
----
ansible-playbook -vv playbooks/aws_ec2_create_vm.yml
----

You should be able to connect to your instance using a command like the following:

[source, bash]
----
ssh -i ~/.ssh/aws-gpu-key.pem cloud-user@$PublicDnsName
----

If you forgot the DNS name, you can use the following command:

[source, bash]
----
aws ec2 describe-instances | jq -r '.Reservations[].Instances[] | "\(.Tags[] | select(.Key == "Name").Value) \(.State.Name) \(.PublicDnsName)"' 
----



== Using the RHEL AI VM

The easiest way to get value from RHEL AI is that you can clone your git repo with your taxonomy and fine-tune the repo using `Instructlab`.


=== Check GPU usage

How can i be sure if the gpu is in use or not under `instructlab` VM?

[source, bash]
----
nvtop
----

The output in the terminal should be something like this:

.Output of the nvtop command
image::nvtop-output.png["Output of the nvtop command"]



=== Disable Red Hat Insights

To run RHEL AI in a disconnected environment, or opt out of Red Hat Insights, run the following commands:

[source, bash]
----
sudo mkdir -p /etc/ilab
sudo touch /etc/ilab/insights-opt-out
----


=== Init instructlab

Initialize InstructLab by running the following command:

[source, bash]
----
ilab config init
----


=== Check instruct lab system info

You can ask `instructlab` to inspect the node and gather all the requirements with the following command:

[source, bash]
----
ilab system info
----

This is the output for my deployment:

[source, text]
----
sys.version: 3.11.7 (main, Aug 23 2024, 00:00:00) [GCC 11.4.1 20231218 (Red Hat 11.4.1-3)]
sys.platform: linux
os.name: posix
platform.release: 5.14.0-427.37.1.el9_4.x86_64
platform.machine: x86_64
platform.node: ip-10-1-1-49.eu-west-1.compute.internal
platform.python_version: 3.11.7
os-release.ID: rhel
os-release.VERSION_ID: 9.4
os-release.PRETTY_NAME: Red Hat Enterprise Linux 9.4 (Plow)
instructlab.version: 0.19.3
instructlab-dolomite.version: 0.1.1
instructlab-eval.version: 0.3.1
instructlab-quantize.version: 0.1.0
instructlab-schema.version: 0.4.1
instructlab-sdg.version: 0.3.0
instructlab-training.version: 0.5.4
torch.version: 2.3.1
torch.backends.cpu.capability: AVX2
torch.version.cuda: 12.4
torch.version.hip: None
torch.cuda.available: True
torch.backends.cuda.is_built: True
torch.backends.mps.is_built: False
torch.backends.mps.is_available: False
torch.cuda.bf16: True
torch.cuda.current.device: 0
torch.cuda.0.name: NVIDIA A10G
torch.cuda.0.free: 21.7 GB
torch.cuda.0.total: 22.0 GB
torch.cuda.0.capability: 8.6 (see https://developer.nvidia.com/cuda-gpus#compute)
llama_cpp_python.version: 0.2.79
llama_cpp_python.supports_gpu_offload: True
----




== Useful links


* https://github.com/instructlab/instructlab
* https://huggingface.co/instructlab 
* https://medium.com/@manojjahgirdar/learn-how-to-train-an-open-source-large-language-model-llm-with-instructlab-part-1-skills-5f64a23a8263
* https://github.com/instructlab/instructlab/blob/main/docs/gpu-acceleration.md#%EF%B8%8F-making-ilab-go-fast
// * https://redhat-internal.slack.com/archives/C072Y48PW91/p1724885516424349?thread_ts=1724838404.729509&cid=C072Y48PW91
* https://github.com/RedHatOfficial/rhelai-dev-preview
* https://github.com/open-webui/open-webui





== Annex: Multiple Python environments in Fedora

Unfortunately, at the time of writing, `torch` does not have GPU-specific support for the latest Python (3.12), so if you're on Linux, it's recommended to set up a Python 3.11-specific `venv` and install `ilab` to that to minimize issues. 

To handle that, I recommend using the https://developer.fedoraproject.org/tech/languages/python/multiple-pythons.html[official guide from the Fedora Project].
